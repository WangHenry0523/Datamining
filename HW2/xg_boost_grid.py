import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import roc_auc_score, log_loss
from xgboost import XGBClassifier, plot_importance
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import featuretools as ft
from featuretools.primitives import MultiplyNumeric, DivideNumeric, AddNumeric, SubtractNumeric
from sklearn.impute import SimpleImputer

# è¼‰å…¥è³‡æ–™
df = pd.read_csv("HW2/ncku-cs-data-mining-homework-2/train_with_features.csv")
df = df.iloc[:, 1:]

# ç‰¹å¾µèˆ‡æ¨™ç±¤
X = df.iloc[:, :-1]
Y = df["smoking"]
# ---------- åˆ©ç”¨ featuretools å»ºç«‹è‡ªå‹•ç‰¹å¾µ ----------
# Step 1: å»ºç«‹ EntitySet
es = ft.EntitySet(id='smoking_data')
es = es.add_dataframe(dataframe_name="health", dataframe=X, index="index", make_index=True)

# Step 2: ä½¿ç”¨ Deep Feature Synthesis å»ºç«‹è‡ªå‹•ç‰¹å¾µ
feature_matrix, feature_defs = ft.dfs(
    entityset=es,
    target_dataframe_name="health",
    trans_primitives=[MultiplyNumeric(), DivideNumeric(), AddNumeric(), SubtractNumeric()],
    max_depth=2  # æ§åˆ¶ç‰¹å¾µè¤‡é›œåº¦
)

print("ğŸ”§ è‡ªå‹•ç”¢ç”Ÿç‰¹å¾µ shape:", feature_matrix.shape)
print("ğŸ§  ç¤ºæ„ç‰¹å¾µ:", feature_matrix.columns[:10].tolist())

# åæ…‹è™•ç†
#skewness = X.skew()
#skewed_features = skewness[abs(skewness) > 1].index.tolist()
#print("åæ…‹ç‰¹å¾µæ¬„ä½ï¼š", skewed_features)

# å°åæ…‹ç‰¹å¾µåš log1p è™•ç†
#X[skewed_features] = X[skewed_features].apply(lambda x: np.log1p(x))
#X["height*hemo"]=X["hemoglobin"]*X["height(cm)"]
#X["height*weight"]=X["weight(kg)"]*X["height(cm)"]
#X=X.drop(["hearing(left)","hearing(right)","relaxation","eyesight(right)","eyesight(left)"],axis=1)
"""
X=df[["height(cm)",
      "hemoglobin",
      "Gtp",
      "triglyceride",
      "dental caries",
      "LDL",
      "BMI",
      "serum creatinine",
      "Cholesterol"]]
"""
X = feature_matrix

print("ç‰¹å¾µ X çš„ shape:", X.shape)
print("æ¨™ç±¤ Y çš„ shape:", Y.shape)

# åˆ†å‰²è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
imputer = SimpleImputer(strategy="mean")  # æˆ– "median"
X_train = imputer.fit_transform(X_train)

# å®šç¾© XGBoost æ¨¡å‹
model = XGBClassifier(
    n_estimators=100,
    max_depth=4,
    min_child_weight=3,
    gamma=0.5,
    subsample=1,
    colsample_bytree=1,
    reg_alpha=0.1,
    reg_lambda=10,
    learning_rate=0.2,
    #use_label_encoder=False, 
    eval_metric='auc', 
    random_state=42
    )

# è¶…åƒæ•¸ç¯„åœ
param_grid = {
    #'n_estimators': [100,150,200,250,300,400,500],

    #'max_depth': [3, 4, 5,6,7],
    #'min_child_weight' : [1,2,3,4,5,6,7],

    #'gamma': [0.3, 0.4, 0.5, 0.6,0.7],

    
    #'subsample': [1,1.2,1.3,1.4],
    #'colsample_bytree': [0.9, 1.0,1.1],

    #'reg_alpha': [0,0.01,0.1,1,10],
    #'reg_lambda': [0,0.01,0.1,1.0,10],

    #'learning_rate': [0.1,0.15,0.2]
}

# è¨­å®š GridSearchCV
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring={'logloss': 'neg_log_loss', 'auc': 'roc_auc'},
    refit='auc',  # è®“ä»–æœ€å¾Œé¸ AUC æœ€ä½³çš„æ¨¡å‹
    cv=skf,
    verbose=1,
    n_jobs=-1
)


# åŸ·è¡Œ Grid Search
grid_search.fit(X_train, y_train)

# æœ€ä½³æ¨¡å‹
print("Best Parameters:", grid_search.best_params_)
print("Best CV log loss:", grid_search.best_score_)

best_model = grid_search.best_estimator_

# æ¸¬è©¦é›†è¡¨ç¾
y_pred_proba = best_model.predict_proba(X_test)[:, 1]
test_log_loss_score = log_loss(y_test, y_pred_proba)
test_auc_score = roc_auc_score(y_test, y_pred_proba)
print(f"XGBoost Test log_loss: {test_log_loss_score:.4f}")
print(f"XGBoost Test AUC: {test_auc_score:.4f}")

# ç•«å‡ºç‰¹å¾µé‡è¦æ€§
plot_importance(best_model, max_num_features=31, importance_type='gain', height=0.5)
plt.title("Top 10 Feature Importances by Gain")
plt.tight_layout()
plt.show()

# è½‰æˆé¡åˆ¥é æ¸¬çµæœï¼ˆ0 or 1ï¼‰
y_pred = best_model.predict(X_test)

# æ··æ·†çŸ©é™£
cm = confusion_matrix(y_test, y_pred)

# é¡¯ç¤ºæ··æ·†çŸ©é™£
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Non-Smoker", "Smoker"])
disp.plot(cmap=plt.cm.Blues)
plt.title("XGBoost Confusion Matrix")
plt.show()

# è¼‰å…¥æ¸¬è©¦è³‡æ–™ï¼Œé€²è¡Œé æ¸¬
testcsv = pd.read_csv("HW2/ncku-cs-data-mining-homework-2/test_with_features.csv")
test_feature = testcsv.iloc[:, 1:]

# æ¸¬è©¦è³‡æ–™ä¹Ÿè¦å»ºç«‹ EntitySet ä¸¦ç”¢ç”Ÿç›¸åŒç‰¹å¾µ
es_test = ft.EntitySet(id='test_data')
es_test = es_test.add_dataframe(dataframe_name="health", dataframe=test_feature, index="index", make_index=True)

test_feature= ft.calculate_feature_matrix(
    features=feature_defs,
    entityset=es_test
)

#test_feature["height*hemo"]=test_feature["hemoglobin"]*test_feature["height(cm)"]
#test_feature["height*weight"]=test_feature["weight(kg)"]*test_feature["height(cm)"]
print("æ¸¬è©¦ç‰¹å¾µçš„å¤§å°:", test_feature.shape)

# é æ¸¬
answer = best_model.predict(test_feature)

# ç”¢ç”Ÿæäº¤æª”æ¡ˆ
submission = pd.DataFrame({
    "id": testcsv.iloc[:, 0],
    "smoking": answer
})
submission.to_csv("HW2/answer_xgboost_grid.csv", index=False)
print("æˆåŠŸç”Ÿæˆ answer_xgboost_grid.csvï¼")
